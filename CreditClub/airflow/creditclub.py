import base64
import json
import os
from datetime import datetime, timedelta
from time import time
from airflow import DAG
from airflow.utils import trigger_rule
from airflow.operators import PythonOperator
from airflow.contrib.operators.dataproc_operator import DataprocClusterCreateOperator, DataProcSparkOperator, DataprocClusterDeleteOperator


dag_id = 'creditclub'.strip()
project_id = 'pid'
default_args = {
  'owner': 'makotoBot98',
  'project_id' : project_id,
  'email_on_failure': False,
  'email_on_retry': False,
  'provide_context': True   # provide to use default args
}

# xcom push func
# push the cluster name
def push_cluster_name(**kwargs):
  # cluster name generated by starting current time
  cluster_name = dag_id + '-' + str(int(round(time() * 100)))
  # get the task instance from the context
  ti = kwargs['ti']
  ti.xcom_push(key = 'cluster_name', value = cluster_name)

# create DAG
with DAG(
  dag_id = dag_id,
  start_date = datetime(2019, 11, 29),
  schedule_interval = '@daily',
  max_active_runs = 1,
  concurrency = 1,
  default_args = default_args
  ) as dag:
  # operators def and dependency: 1.push cluster name through xcom -> 2.create cluster -> 3.run credit analysis spark job -> 4.delete cluster
  push_cluster_name_op = PythonOperator(
    task_id = "push_cluster_name",
    python_callable = push_cluster_name,
    dag = dag
  )
  create_dataproc_cluster = DataprocClusterCreateOperator(
    task_id = 'create_dataproc_cluster',
    project_id = project_id,
    region = 'us-west1',
    master_machine_type = 'n1-standard-2',
    worker_machine_type = 'n1-standard-2',
    num_workers = 2,
    cluster_name = '{{ ti.xcom_pull(key = "cluster_name", task_ids = "push_cluster_name") }}' #get the cluster name from xcom in template
  )
  run_collection_analysis_job = DataProcSparkOperator(
    task_id = 'start_collection_analysis_spark_job',
    main_class = 'com.makoto.spark.LoanAnalyze',
    dataproc_spark_jars = "gs://creditclub/CreditClub-assembly-0.1.jar",
    arguments = [     
      "input_load_stats_csv_path",
      "input_rejection_stats_csv_path",
      "output_path"    
    ],
    job_name = 'creditanalysis',
    region = 'us-west1',
    cluster_name = '{{ ti.xcom_pull(key = "cluster_name", task_ids = "push_cluster_name") }}'
  )
  delete_dataproc_cluster = DataprocClusterDeleteOperator(
    task_id = 'delete_dataproc_cluster',
    project_id = project_id,
    cluster_name = '{{ ti.xcom_pull(key = "cluster_name", task_ids = "push_cluster_name") }}',
    region = 'us-west1',
    trigger_rule = trigger_rule.TriggerRule.ALL_DONE
  )
  # dependency
  push_cluster_name_op >> create_dataproc_cluster >> run_collection_analysis_job >> delete_dataproc_cluster

